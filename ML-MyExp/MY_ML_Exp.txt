1. Making computers learn without explicitly programming them.

2. Two types, Supervised, Unsupervised
	Supervised- we are given a data set and already know what our correct output should look like, having the idea that there is a relationship between the input and the output.

	Unsupervised-we approach problems with little or no idea what our results should look like. We can derive structure from data where we don't necessarily know the effect of the variables.

3. Supervised- Regression, Classification

4. In Supervised learning, the aim is to find a function h, called hypothesis that maps the inputs to the outputs in such a way that it is highly likely that a new input(outside of training set), 
   will result in correct output when passed through h

5. Cost func, C=1/2 mse. mse=mean sqrd error, we must choose an h that minimises this C, mse= [(h(x)-y)^2]/m. 

6. Alternatively, if we plot contour plot b/w say theta1 and theta0(say h=theta0 +theta1*x), then the center of the contour plot will give the minimal cost add hence the correct values of theta0 and theta1 that minimizes C

7. We need to learn the parameters in the hypothesis h. One way is gradient descent, thetaj=thetaj-alpha*(dC/dthetaj), alpha is the learning rate, determines the step size. 
   The problem: when alpha is too small, time to converge may be high. If alpha is too high, u may fail to converge, rather may diverge. 
   We can repeat until thetaj value does not change significantly, then we have reached a local minimum.

8. Problem with gradient descent is that we may not reach global minima if there are local minima available.

9. Multi-Variable linear regression, that is when there are more than one inputs, then h=t0+t1*x1+t2*x2...+tn*xn=T'X, gradient descent is peformed similarly using simulataneous update of Thetaz.

10. Usually before applying any ML algorithm like Gradient descent, we must pre-process the incoming data. Some of the preprocessing techniques are feature scaling and mean normalisation.

11. Feature scaling involves dividing the input values by the range (i.e. the maximum value minus the minimum value) of the input variable, resulting in a new range of just 1. 

12. Mean normalization involves subtracting the average value for an input variable from the values for that input variable resulting in a new average value for the input variable of just zero.

13. 11 and 12 can be achieved as x= x-mu / r, where x is any feature, mu is the average value for that feature, and r is range for that feature.

14. If we choose a hypothesis h, which is non-linear, then we must make sure that feature scaling anad mean normalisation are carried out before training.

15. We can directly solve for T values using normal equation T= (X'*X)^(-1) * X' * y, this doe snot need any feature scaling.

16. Normal equation has complexity O(n^3) whereas Gradient descent has complexity O(k*n^2), so if features n are large GD will perform better.
    Also normal equation can fail if (X' * X) is non-invertible, in that case we need to find out linearly dependent features and remove them.
    
