1. Making computers learn without explicitly programming them.

2. Two types, Supervised, Unsupervised
	Supervised- we are given a data set and already know what our correct output should look like, having the idea that there is a relationship between the input and the output.

	Unsupervised-we approach problems with little or no idea what our results should look like. We can derive structure from data where we don't necessarily know the effect of the variables.

3. Supervised- Regression, Classification

4. In Supervised learning the aim is to find a function h, called hypothesis that maps the inputs to the outputs in such a way that it is highly likely that a new input(outside of training set), will result in correct output, when passed through h

5. Cost func, C=1/2 mse. mse=mean sqrd error, we must choose an h that minimises this C, mse= [(h(x)-y)^2]/m. 

6. Alternatively, if we plot contour plot b/w say theta1 and theta0(say h=theta0 +theta1*x), then the centre of the contour plot will give the minimal cost add hence the correct values of theta0 and theta1 taht minimises C

7.We need to learn the parameters in the hypothesis h. One way is gradient descent, thetaj=thetaj-alpha*(dC/dthetaj), alpha is the learning rate, determines the step size. The problem: when alpha is too small, time to converge may be high. If alpha is too high, u may fail to converge, rather may diverge. We can repeat until thetaj value does not change significantly, then we have reached a local minima.

8. Problem with gradient descent is that we may not reach global minima, if there are lcoal minima available.

9.Multi-Variable linear regression, that is when there are more than one inputs, then h=t0+t1*x1+t2*x2...+tn*xn=T'X, gradient descent is peformed similarly using simulataneous update of Thetaz.

10. Usually before applying any ML algorithm like Gradient descent, we must prerprocess the incoming data. Come of the preprocessing techniques are feature scaling nd mean normalisation.

11. Feature scaling involves dividing the input values by the range (i.e. the maximum value minus the minimum value) of the input variable, resulting in a new range of just 1. 

12. Mean normalization involves subtracting the average value for an input variable from the values for that input variable resulting in a new average value for the input variable of just zero.

13. 11 and 12 can be achieved as x= x-mu / r, where x is any feature, mu is the average value for that feature, and r is range for that feature.

14.
