1. Making computers learn without explicitly programming them.

2. Two types, Supervised, Unsupervised
	Supervised- we are given a data set and already know what our correct output should look like, having the idea that there is a relationship between the input and the output.

	Unsupervised-we approach problems with little or no idea what our results should look like. We can derive structure from data where we don't necessarily know the effect of the variables.

3. Supervised- Regression, Classification

4. In Supervised learning the aim is to find a function h, called hypothesis that maps the inputs to the outputs in such a way that it is highly likely that a new input(outside of training set), will result in correct output, when passed through h

5. Cost func, C=1/2 mse. mse=mean sqrd error, we must choose an h that minimises this C, mse= [(h(x)-y)^2]/m. 

6. Alternatively, if we plot contour plot b/w say theta1 and theta0(say h=theta0 +theta1*x), then the centre of the contour plot will give the minimal cost add hence the correct values of theta0 and theta1 taht minimises C

7.We need to learn the parameters in the hypothesis h. One way is gradient descent, thetaj=thetaj-alpha*(dC/dthetaj), alpha is the learning rate, determines the step size. The problem: when alpha is too small, time to converge may be high. If alpha is too high, u may fail to converge, rather may diverge. We can repeat until thetaj value does not change significantly, then we have reached a local minima.

8. Problem with fradient descent is that we may not reach global minima, if there are lcoal minima available.

9.